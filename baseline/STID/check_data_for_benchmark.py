import os
import json
import pickle
import numpy as np

# æ•°æ®é›†é…ç½®ï¼ˆä¸ prepare_data_for_stid.py ä¸€è‡´ï¼‰
datasets = [
    {
        "name": "PEMS03",
        "original_shape": (26208, 358, 1),
        "expected_features": 3,  # 1ä¸ªåŸå§‹ç‰¹å¾ + 2ä¸ªæ—¶é—´ç‰¹å¾
        "steps_per_day": 288,
        "traffic_max": 10000,  # å‡è®¾æµé‡æœ€å¤§å€¼ï¼ˆæ ¹æ® PEMS æ•°æ®é›†å¸¸è¯†ï¼‰
        "feature_ranges": {
            "traffic_flow": (0, 10000),  # æµé‡èŒƒå›´
        }
    },
    {
        "name": "PEMS04",
        "original_shape": (16992, 307, 3),
        "expected_features": 5,  # 3ä¸ªåŸå§‹ç‰¹å¾ + 2ä¸ªæ—¶é—´ç‰¹å¾
        "steps_per_day": 288,
        "traffic_max": 10000,
        "feature_ranges": {
            "traffic_flow": (0, 10000),
            "occupancy": (0, 100),  # å ç”¨ç‡é€šå¸¸ä¸ºç™¾åˆ†æ¯”
            "speed": (0, 120),  # é€Ÿåº¦ï¼ˆkm/hï¼‰ï¼Œå‡è®¾æœ€å¤§ä¸º 120
        }
    },
    {
        "name": "PEMS08",
        "original_shape": (17856, 170, 3),
        "expected_features": 5,  # 3ä¸ªåŸå§‹ç‰¹å¾ + 2ä¸ªæ—¶é—´ç‰¹å¾
        "steps_per_day": 288,
        "traffic_max": 10000,
        "feature_ranges": {
            "traffic_flow": (0, 10000),
            "occupancy": (0, 100),
            "speed": (0, 120),
        }
    },
    {
        "name": "NYCBike2_part1",
        "original_shape": (11952, 200, 4),
        "expected_features": 6,  # 4ä¸ªåŸå§‹ç‰¹å¾ + 2ä¸ªæ—¶é—´ç‰¹å¾
        "steps_per_day": 48,
        "traffic_max": 500,  # å…±äº«å•è½¦æµé‡è¾ƒå°ï¼Œå‡è®¾æœ€å¤§ä¸º 500
        "feature_ranges": {
            "traffic_flow": (0, 500),
            "unknown1": (0, float('inf')),  # æœªçŸ¥ç‰¹å¾ï¼Œæš‚ä¸é™åˆ¶
            "unknown2": (0, float('inf')),
            "unknown3": (0, float('inf')),
        }
    },
    {
        "name": "NYCTaxi_part1",
        "original_shape": (11952, 200, 3),
        "expected_features": 5,  # 3ä¸ªåŸå§‹ç‰¹å¾ + 2ä¸ªæ—¶é—´ç‰¹å¾
        "steps_per_day": 48,
        "traffic_max": 1000,  # å‡ºç§Ÿè½¦æµé‡ï¼Œå‡è®¾æœ€å¤§ä¸º 1000
        "feature_ranges": {
            "traffic_flow": (0, 1000),
            "unknown1": (0, float('inf')),
            "unknown2": (0, float('inf')),
        }
    },
    {
        "name": "Taxi_BJ_hist12_pred12_group1",
        "original_shape": (14163, 128, 4),
        "expected_features": 6,  # 4ä¸ªåŸå§‹ç‰¹å¾ + 2ä¸ªæ—¶é—´ç‰¹å¾
        "steps_per_day": 48,
        "traffic_max": 1000,
        "feature_ranges": {
            "traffic_flow": (0, 1000),
            "temperature": (-30, 50),  # åŒ—äº¬æ¸©åº¦èŒƒå›´ï¼ˆæ‘„æ°åº¦ï¼‰
            "wind_speed": (0, 30),  # é£é€Ÿï¼ˆm/sï¼‰ï¼Œå‡è®¾æœ€å¤§ä¸º 30
            "weather_index": (0, 10),  # å¤©æ°”ç±»å‹ç´¢å¼•ï¼Œå‡è®¾ä¸º 0-10
        }
    }
]

# æ•°æ®ç›®å½•
data_base_dir = "/root/python_on_hyy/data_for_benchmark"

def check_files_exist(dataset):
    """æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    dataset_dir = os.path.join(data_base_dir, dataset["name"])
    required_files = ["data.dat", "adj_mx.pkl", "desc.json"]
    
    print(f"\nChecking files for {dataset['name']}...")
    for file_name in required_files:
        file_path = os.path.join(dataset_dir, file_name)
        if not os.path.exists(file_path):
            print(f"âŒ Error: {file_name} is missing in {dataset_dir}")
            return False
        else:
            print(f"âœ” {file_name} exists")
    return True

def check_data_shape(dataset):
    """æ£€æŸ¥ data.dat çš„å½¢çŠ¶"""
    dataset_dir = os.path.join(data_base_dir, dataset["name"])
    desc_path = os.path.join(dataset_dir, "desc.json")
    
    # è¯»å–æè¿°æ–‡ä»¶
    with open(desc_path, "r") as f:
        desc = json.load(f)
    
    data_shape = tuple(desc["shape"])
    expected_shape = (dataset["original_shape"][0], dataset["original_shape"][1], dataset["expected_features"])
    
    print(f"\nChecking data shape for {dataset['name']}...")
    print(f"Expected shape: {expected_shape}")
    print(f"Actual shape: {data_shape}")
    
    if data_shape != expected_shape:
        print(f"âŒ Error: Data shape mismatch for {dataset['name']}")
        return False
    
    # åŠ è½½æ•°æ®å¹¶è¿›ä¸€æ­¥æ£€æŸ¥
    data_path = os.path.join(dataset_dir, "data.dat")
    data = np.memmap(data_path, dtype="float32", mode="r", shape=data_shape)
    print(f"âœ” Data shape matches expected shape")
    return data

def check_temporal_features(dataset, data):
    """æ£€æŸ¥æ—¶é—´ç‰¹å¾ time_of_day å’Œ day_of_week"""
    T, N, C = data.shape
    steps_per_day = dataset["steps_per_day"]
    
    print(f"\nChecking temporal features for {dataset['name']}...")
    
    # æ£€æŸ¥ time_of_dayï¼ˆå€’æ•°ç¬¬äºŒä¸ªç‰¹å¾ï¼‰
    time_of_day = data[:, 0, -2]  # å–ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„æ—¶é—´ç‰¹å¾
    expected_time_of_day = np.array([i % steps_per_day / steps_per_day for i in range(T)])
    
    if not np.allclose(time_of_day, expected_time_of_day, atol=1e-5):
        print(f"âŒ Error: time_of_day feature is incorrect for {dataset['name']}")
        return False
    print(f"âœ” time_of_day feature is correct")
    
    # æ£€æŸ¥ day_of_weekï¼ˆæœ€åä¸€ä¸ªç‰¹å¾ï¼‰
    day_of_week = data[:, 0, -1]  # å–ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„æ—¶é—´ç‰¹å¾
    expected_day_of_week = np.array([(i // steps_per_day) % 7 / 7 for i in range(T)])
    
    if not np.allclose(day_of_week, expected_day_of_week, atol=1e-5):
        print(f"âŒ Error: day_of_week feature is incorrect for {dataset['name']}")
        return False
    print(f"âœ” day_of_week feature is correct")
    
    # æ£€æŸ¥æ—¶é—´ç‰¹å¾åˆ†å¸ƒï¼ˆç¡®ä¿å‡åŒ€æ€§ï¼‰
    time_of_day_bins = np.histogram(time_of_day, bins=10, range=(0, 1))[0]
    expected_bin_count = T / 10  # å‡è®¾å‡åŒ€åˆ†å¸ƒ
    if np.any(np.abs(time_of_day_bins - expected_bin_count) > 0.1 * expected_bin_count):
        print(f"âŒ Warning: time_of_day distribution is not uniform for {dataset['name']}")
        print(f"Bin counts: {time_of_day_bins}")
    else:
        print(f"âœ” time_of_day distribution is uniform")
    
    day_of_week_bins = np.histogram(day_of_week, bins=7, range=(0, 1))[0]
    expected_bin_count = T / 7
    if np.any(np.abs(day_of_week_bins - expected_bin_count) > 0.1 * expected_bin_count):
        print(f"âŒ Warning: day_of_week distribution is not uniform for {dataset['name']}")
        print(f"Bin counts: {day_of_week_bins}")
    else:
        print(f"âœ” day_of_week distribution is uniform")
    
    return True

def check_adj_matrix(dataset):
    """æ£€æŸ¥é‚»æ¥çŸ©é˜µ adj_mx.pkl"""
    dataset_dir = os.path.join(data_base_dir, dataset["name"])
    adj_path = os.path.join(dataset_dir, "adj_mx.pkl")
    
    with open(adj_path, "rb") as f:
        adj_data = pickle.load(f)
    
    # adj_mx.pkl åŒ…å« [node_ids, node_ids, adj_matrix]
    adj_matrix = adj_data[2]
    expected_shape = (dataset["original_shape"][1], dataset["original_shape"][1])
    
    print(f"\nChecking adjacency matrix for {dataset['name']}...")
    print(f"Expected shape: {expected_shape}")
    print(f"Actual shape: {adj_matrix.shape}")
    
    if adj_matrix.shape != expected_shape:
        print(f"âŒ Error: Adjacency matrix shape mismatch for {dataset['name']}")
        return False
    print(f"âœ” Adjacency matrix shape is correct")
    
    # æ£€æŸ¥å¯¹ç§°æ€§ï¼ˆæ— å‘å›¾ï¼‰
    if not np.allclose(adj_matrix, adj_matrix.T, atol=1e-5):
        print(f"âŒ Error: Adjacency matrix is not symmetric for {dataset['name']}")
        return False
    print(f"âœ” Adjacency matrix is symmetric")
    
    # æ£€æŸ¥å¯¹è§’çº¿å…ƒç´ ï¼ˆåº”ä¸º 0ï¼‰
    if not np.allclose(np.diag(adj_matrix), 0, atol=1e-5):
        print(f"âŒ Error: Diagonal elements of adjacency matrix are not zero for {dataset['name']}")
        return False
    print(f"âœ” Diagonal elements are zero")
    
    # æ£€æŸ¥è¾¹çš„æƒé‡èŒƒå›´ï¼ˆå‡è®¾ cost ä¸ºè·ç¦»æˆ–æƒé‡ï¼Œåº”ä¸ºéè´Ÿä¸”åˆç†ï¼‰
    non_zero_weights = adj_matrix[adj_matrix > 0]
    if len(non_zero_weights) == 0:
        print(f"âŒ Error: Adjacency matrix has no edges for {dataset['name']}")
        return False
    
    if np.any(non_zero_weights < 0):
        print(f"âŒ Error: Adjacency matrix contains negative weights for {dataset['name']}")
        return False
    print(f"âœ” No negative weights in adjacency matrix")
    
    # å‡è®¾æœ€å¤§æƒé‡ä¸º 100ï¼ˆæ ¹æ® PEMS æ•°æ®é›†çš„ cost é€šå¸¸ä¸ºè·ç¦»ï¼‰
    max_weight = 100
    if np.any(non_zero_weights > max_weight):
        print(f"âŒ Error: Adjacency matrix contains weights larger than {max_weight} for {dataset['name']}")
        return False
    print(f"âœ” Weights are within reasonable range [0, {max_weight}]")
    
    # æ£€æŸ¥ç¨€ç–æ€§ï¼ˆäº¤é€šç½‘ç»œé€šå¸¸ç¨€ç–ï¼Œè¾¹çš„æ•°é‡åº”è¿œå°äº N^2ï¼‰
    N = adj_matrix.shape[0]
    num_edges = np.sum(adj_matrix > 0)
    sparsity = num_edges / (N * N)
    if sparsity > 0.1:  # å‡è®¾ç¨€ç–æ€§é˜ˆå€¼ä¸º 10%
        print(f"âŒ Warning: Adjacency matrix is too dense for {dataset['name']}, sparsity = {sparsity:.4f}")
    else:
        print(f"âœ” Adjacency matrix is sparse, sparsity = {sparsity:.4f}")
    
    return True

def check_description(dataset):
    """æ£€æŸ¥ desc.json çš„å…ƒä¿¡æ¯"""
    dataset_dir = os.path.join(data_base_dir, dataset["name"])
    desc_path = os.path.join(dataset_dir, "desc.json")
    
    with open(desc_path, "r") as f:
        desc = json.load(f)
    
    print(f"\nChecking description file for {dataset['name']}...")
    
    # æ£€æŸ¥ shape
    expected_shape = (dataset["original_shape"][0], dataset["original_shape"][1], dataset["expected_features"])
    if tuple(desc["shape"]) != expected_shape:
        print(f"âŒ Error: Shape in desc.json does not match expected shape for {dataset['name']}")
        return False
    print(f"âœ” Shape in desc.json is correct")
    
    # æ£€æŸ¥ num_nodes
    if desc["num_nodes"] != dataset["original_shape"][1]:
        print(f"âŒ Error: num_nodes in desc.json does not match expected value for {dataset['name']}")
        return False
    print(f"âœ” num_nodes in desc.json is correct")
    
    # æ£€æŸ¥ num_features
    if desc["num_features"] != dataset["expected_features"]:
        print(f"âŒ Error: num_features in desc.json does not match expected value for {dataset['name']}")
        return False
    print(f"âœ” num_features in desc.json is correct")
    
    # æ£€æŸ¥ feature_description
    expected_features = ["traffic_flow"] + ["unknown" + str(i) for i in range(1, dataset["original_shape"][2])] + ["time_of_day", "day_of_week"]
    if dataset["name"] == "PEMS04" or dataset["name"] == "PEMS08":
        expected_features = ["traffic_flow", "occupancy", "speed", "time_of_day", "day_of_week"]
    elif dataset["name"] == "Taxi_BJ_hist12_pred12_group1":
        expected_features = ["traffic_flow", "temperature", "wind_speed", "weather_index", "time_of_day", "day_of_week"]
    
    if desc["feature_description"] != expected_features:
        print(f"âŒ Error: feature_description in desc.json does not match expected value for {dataset['name']}")
        print(f"Expected: {expected_features}")
        print(f"Actual: {desc['feature_description']}")
        return False
    print(f"âœ” feature_description in desc.json is correct")
    return True

def check_data_values(dataset, data):
    """æ£€æŸ¥æ•°æ®å€¼èŒƒå›´"""
    T, N, C = data.shape
    print(f"\nChecking data values for {dataset['name']}...")
    
    # æ£€æŸ¥æ¯ä¸ªç‰¹å¾çš„èŒƒå›´
    for feature_idx, feature_name in enumerate(dataset["feature_ranges"].keys()):
        feature_data = data[:, :, feature_idx]
        min_val, max_val = dataset["feature_ranges"][feature_name]
        
        if np.any(feature_data < min_val) or np.any(feature_data > max_val):
            print(f"âŒ Error: {feature_name} values out of range [{min_val}, {max_val}] in {dataset['name']}")
            print(f"Min value: {np.min(feature_data)}, Max value: {np.max(feature_data)}")
            return False
        print(f"âœ” {feature_name} values are within range [{min_val}, {max_val}]")
    
    # æ£€æŸ¥æ—¶é—´ç‰¹å¾èŒƒå›´
    time_of_day = data[:, :, -2]
    day_of_week = data[:, :, -1]
    
    if np.any(time_of_day < 0) or np.any(time_of_day > 1):
        print(f"âŒ Error: time_of_day values out of range [0, 1] in {dataset['name']}")
        return False
    print(f"âœ” time_of_day values are within range [0, 1]")
    
    if np.any(day_of_week < 0) or np.any(day_of_week > 1):
        print(f"âŒ Error: day_of_week values out of range [0, 1] in {dataset['name']}")
        return False
    print(f"âœ” day_of_week values are within range [0, 1]")
    
    # æ£€æŸ¥æµé‡å€¼çš„åˆ†å¸ƒï¼ˆé¿å…å¼‚å¸¸é›†ä¸­ï¼‰
    traffic_flow = data[:, :, 0]
    traffic_bins = np.histogram(traffic_flow, bins=10, range=(0, dataset["traffic_max"]))[0]
    if np.any(traffic_bins == 0):
        print(f"âŒ Warning: Traffic flow distribution has empty bins for {dataset['name']}")
        print(f"Bin counts: {traffic_bins}")
    else:
        print(f"âœ” Traffic flow distribution is reasonable")
    
    return True

def main():
    all_passed = True
    for dataset in datasets:
        print(f"\n=== Checking dataset: {dataset['name']} ===")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not check_files_exist(dataset):
            all_passed = False
            continue
        
        # æ£€æŸ¥æ•°æ®å½¢çŠ¶
        data = check_data_shape(dataset)
        if data is False:
            all_passed = False
            continue
        
        # æ£€æŸ¥æ—¶é—´ç‰¹å¾
        if not check_temporal_features(dataset, data):
            all_passed = False
            continue
        
        # æ£€æŸ¥é‚»æ¥çŸ©é˜µ
        if not check_adj_matrix(dataset):
            all_passed = False
            continue
        
        # æ£€æŸ¥æè¿°æ–‡ä»¶
        if not check_description(dataset):
            all_passed = False
            continue
        
        # æ£€æŸ¥æ•°æ®å€¼èŒƒå›´
        if not check_data_values(dataset, data):
            all_passed = False
            continue
        
        print(f"\nâœ… All checks passed for {dataset['name']}")
    
    if all_passed:
        print("\nğŸ‰ All datasets passed the checks!")
    else:
        print("\nâŒ Some datasets failed the checks. Please review the errors above.")

if __name__ == "__main__":
    main()